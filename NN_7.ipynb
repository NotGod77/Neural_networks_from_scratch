{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Backpropagation on ONE neuron"
      ],
      "metadata": {
        "id": "0yuUYRsPQXEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "iaa5L-zEt6Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swdNv0d0OX7-"
      },
      "outputs": [],
      "source": [
        "weights = np.array([-3.0,-1.0,2.0])\n",
        "bias = 1.0\n",
        "inputs = np.array([1.0,-2.0,3.0])\n",
        "target_output = 0.0\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "  return np.where(x>0,1.0,0.0)"
      ],
      "metadata": {
        "id": "etdgPwqBBujG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iteration in range (200):\n",
        "  linear_output = np.dot(weights, inputs)+bias\n",
        "  output = relu(linear_output)\n",
        "  loss = (output - target_output)**2\n",
        "\n",
        "  #derivative of loss wrt weights = 4 terms in notebook\n",
        "  # 3rd term is always 1\n",
        "\n",
        "  #here output is the relu(sum())\n",
        "  #linnear is sum()\n",
        "\n",
        "  dloss_doutput = 2*(output-target_output) #1st term\n",
        "  doutput_dlinear = relu_derivative(linear_output) #2nd term\n",
        "  dlinear_dweights = inputs #4th term\n",
        "\n",
        "  dlinear_dbias = 1 #identical to the 3rd term of dloss_dweights\n",
        "\n",
        "  dloss_dweights = dloss_doutput*doutput_dlinear*dlinear_dweights\n",
        "  dloss_dbias = dloss_doutput*doutput_dlinear*dlinear_dbias\n",
        "\n",
        "  #Update the weights and biases\n",
        "  weights -= learning_rate*dloss_dweights\n",
        "  bias -= learning_rate*dloss_dbias\n",
        "\n",
        "  #print loss for each iteration\n",
        "  print(f\"Iteration{iteration+1}, Loss: {loss}\")\n",
        "\n",
        "print(\"Final weights: \",weights)\n",
        "print(\"Final bias: \", bias)\n",
        "\n"
      ],
      "metadata": {
        "id": "aCTrC_VtBv8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae7e99dd-c103-409c-af70-19c322b8c4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration1, Loss: 36.0\n",
            "Iteration2, Loss: 33.872399999999985\n",
            "Iteration3, Loss: 31.870541159999995\n",
            "Iteration4, Loss: 29.98699217744401\n",
            "Iteration5, Loss: 28.21476093975706\n",
            "Iteration6, Loss: 26.54726856821742\n",
            "Iteration7, Loss: 24.978324995835766\n",
            "Iteration8, Loss: 23.502105988581878\n",
            "Iteration9, Loss: 22.113131524656684\n",
            "Iteration10, Loss: 20.80624545154949\n",
            "Iteration11, Loss: 19.576596345362915\n",
            "Iteration12, Loss: 18.419619501351963\n",
            "Iteration13, Loss: 17.331019988822064\n",
            "Iteration14, Loss: 16.306756707482677\n",
            "Iteration15, Loss: 15.343027386070442\n",
            "Iteration16, Loss: 14.43625446755368\n",
            "Iteration17, Loss: 13.583071828521268\n",
            "Iteration18, Loss: 12.780312283455652\n",
            "Iteration19, Loss: 12.024995827503426\n",
            "Iteration20, Loss: 11.314318574097976\n",
            "Iteration21, Loss: 10.645642346368787\n",
            "Iteration22, Loss: 10.016484883698395\n",
            "Iteration23, Loss: 9.424510627071816\n",
            "Iteration24, Loss: 8.867522049011871\n",
            "Iteration25, Loss: 8.34345149591527\n",
            "Iteration26, Loss: 7.850353512506679\n",
            "Iteration27, Loss: 7.386397619917536\n",
            "Iteration28, Loss: 6.949861520580408\n",
            "Iteration29, Loss: 6.539124704714106\n",
            "Iteration30, Loss: 6.152662434665503\n",
            "Iteration31, Loss: 5.7890400847767705\n",
            "Iteration32, Loss: 5.446907815766464\n",
            "Iteration33, Loss: 5.124995563854671\n",
            "Iteration34, Loss: 4.8221083260308575\n",
            "Iteration35, Loss: 4.537121723962434\n",
            "Iteration36, Loss: 4.268977830076255\n",
            "Iteration37, Loss: 4.016681240318748\n",
            "Iteration38, Loss: 3.7792953790159096\n",
            "Iteration39, Loss: 3.5559390221160707\n",
            "Iteration40, Loss: 3.345783025909011\n",
            "Iteration41, Loss: 3.148047249077789\n",
            "Iteration42, Loss: 2.9619976566572896\n",
            "Iteration43, Loss: 2.786943595148845\n",
            "Iteration44, Loss: 2.622235228675549\n",
            "Iteration45, Loss: 2.4672611266608238\n",
            "Iteration46, Loss: 2.321445994075166\n",
            "Iteration47, Loss: 2.1842485358253256\n",
            "Iteration48, Loss: 2.0551594473580463\n",
            "Iteration49, Loss: 1.9336995240191863\n",
            "Iteration50, Loss: 1.8194178821496518\n",
            "Iteration51, Loss: 1.7118902853146067\n",
            "Iteration52, Loss: 1.6107175694525138\n",
            "Iteration53, Loss: 1.515524161097869\n",
            "Iteration54, Loss: 1.4259566831769857\n",
            "Iteration55, Loss: 1.3416826432012259\n",
            "Iteration56, Loss: 1.2623891989880334\n",
            "Iteration57, Loss: 1.18778199732784\n",
            "Iteration58, Loss: 1.1175840812857634\n",
            "Iteration59, Loss: 1.0515348620817766\n",
            "Iteration60, Loss: 0.9893891517327431\n",
            "Iteration61, Loss: 0.930916252865338\n",
            "Iteration62, Loss: 0.8758991023209968\n",
            "Iteration63, Loss: 0.8241334653738256\n",
            "Iteration64, Loss: 0.7754271775702323\n",
            "Iteration65, Loss: 0.7295994313758316\n",
            "Iteration66, Loss: 0.6864801049815187\n",
            "Iteration67, Loss: 0.6459091307771115\n",
            "Iteration68, Loss: 0.6077359011481847\n",
            "Iteration69, Loss: 0.571818709390327\n",
            "Iteration70, Loss: 0.5380242236653578\n",
            "Iteration71, Loss: 0.5062269920467349\n",
            "Iteration72, Loss: 0.47630897681677353\n",
            "Iteration73, Loss: 0.4481591162869011\n",
            "Iteration74, Loss: 0.4216729125143454\n",
            "Iteration75, Loss: 0.3967520433847474\n",
            "Iteration76, Loss: 0.3733039976207088\n",
            "Iteration77, Loss: 0.35124173136132436\n",
            "Iteration78, Loss: 0.3304833450378702\n",
            "Iteration79, Loss: 0.3109517793461322\n",
            "Iteration80, Loss: 0.29257452918677535\n",
            "Iteration81, Loss: 0.27528337451183676\n",
            "Iteration82, Loss: 0.25901412707818716\n",
            "Iteration83, Loss: 0.24370639216786655\n",
            "Iteration84, Loss: 0.22930334439074554\n",
            "Iteration85, Loss: 0.21575151673725296\n",
            "Iteration86, Loss: 0.2030006020980815\n",
            "Iteration87, Loss: 0.1910032665140846\n",
            "Iteration88, Loss: 0.17971497346310225\n",
            "Iteration89, Loss: 0.16909381853143338\n",
            "Iteration90, Loss: 0.1591003738562249\n",
            "Iteration91, Loss: 0.14969754176132236\n",
            "Iteration92, Loss: 0.14085041704322837\n",
            "Iteration93, Loss: 0.13252615739597357\n",
            "Iteration94, Loss: 0.12469386149387143\n",
            "Iteration95, Loss: 0.11732445427958357\n",
            "Iteration96, Loss: 0.1103905790316602\n",
            "Iteration97, Loss: 0.1038664958108892\n",
            "Iteration98, Loss: 0.09772798590846558\n",
            "Iteration99, Loss: 0.09195226194127534\n",
            "Iteration100, Loss: 0.08651788326054576\n",
            "Iteration101, Loss: 0.08140467635984766\n",
            "Iteration102, Loss: 0.07659365998698062\n",
            "Iteration103, Loss: 0.07206697468175022\n",
            "Iteration104, Loss: 0.06780781647805834\n",
            "Iteration105, Loss: 0.06380037452420508\n",
            "Iteration106, Loss: 0.06002977238982451\n",
            "Iteration107, Loss: 0.056482012841585764\n",
            "Iteration108, Loss: 0.05314392588264784\n",
            "Iteration109, Loss: 0.050003119862983315\n",
            "Iteration110, Loss: 0.04704793547908108\n",
            "Iteration111, Loss: 0.044267402492267266\n",
            "Iteration112, Loss: 0.04165119900497404\n",
            "Iteration113, Loss: 0.03918961314378035\n",
            "Iteration114, Loss: 0.036873507006982977\n",
            "Iteration115, Loss: 0.034694282742870286\n",
            "Iteration116, Loss: 0.032643850632766785\n",
            "Iteration117, Loss: 0.030714599060370322\n",
            "Iteration118, Loss: 0.028899366255902493\n",
            "Iteration119, Loss: 0.027191413710178584\n",
            "Iteration120, Loss: 0.025584401159906914\n",
            "Iteration121, Loss: 0.024072363051356495\n",
            "Iteration122, Loss: 0.02264968639502138\n",
            "Iteration123, Loss: 0.021311089929075627\n",
            "Iteration124, Loss: 0.02005160451426725\n",
            "Iteration125, Loss: 0.018866554687474075\n",
            "Iteration126, Loss: 0.017751541305444478\n",
            "Iteration127, Loss: 0.016702425214292563\n",
            "Iteration128, Loss: 0.015715311884128023\n",
            "Iteration129, Loss: 0.014786536951776086\n",
            "Iteration130, Loss: 0.013912652617925996\n",
            "Iteration131, Loss: 0.013090414848206543\n",
            "Iteration132, Loss: 0.012316771330677616\n",
            "Iteration133, Loss: 0.011588850145034585\n",
            "Iteration134, Loss: 0.010903949101463065\n",
            "Iteration135, Loss: 0.010259525709566468\n",
            "Iteration136, Loss: 0.00965318774013127\n",
            "Iteration137, Loss: 0.009082684344689475\n",
            "Iteration138, Loss: 0.008545897699918217\n",
            "Iteration139, Loss: 0.008040835145853157\n",
            "Iteration140, Loss: 0.007565621788733219\n",
            "Iteration141, Loss: 0.0071184935410191314\n",
            "Iteration142, Loss: 0.0066977905727448606\n",
            "Iteration143, Loss: 0.0063019511498957235\n",
            "Iteration144, Loss: 0.0059295058369368625\n",
            "Iteration145, Loss: 0.005579072041973911\n",
            "Iteration146, Loss: 0.005249348884293189\n",
            "Iteration147, Loss: 0.004939112365231465\n",
            "Iteration148, Loss: 0.004647210824446307\n",
            "Iteration149, Loss: 0.004372560664721486\n",
            "Iteration150, Loss: 0.004114142329436494\n",
            "Iteration151, Loss: 0.003870996517766834\n",
            "Iteration152, Loss: 0.0036422206235667827\n",
            "Iteration153, Loss: 0.003426965384714017\n",
            "Iteration154, Loss: 0.0032244317304774505\n",
            "Iteration155, Loss: 0.0030338678152062068\n",
            "Iteration156, Loss: 0.0028545662273275238\n",
            "Iteration157, Loss: 0.002685861363292443\n",
            "Iteration158, Loss: 0.002527126956721865\n",
            "Iteration159, Loss: 0.0023777737535795864\n",
            "Iteration160, Loss: 0.00223724732474303\n",
            "Iteration161, Loss: 0.0021050260078507234\n",
            "Iteration162, Loss: 0.0019806189707867374\n",
            "Iteration163, Loss: 0.0018635643896132343\n",
            "Iteration164, Loss: 0.0017534277341871227\n",
            "Iteration165, Loss: 0.001649800155096641\n",
            "Iteration166, Loss: 0.0015522969659304577\n",
            "Iteration167, Loss: 0.001460556215243966\n",
            "Iteration168, Loss: 0.0013742373429230384\n",
            "Iteration169, Loss: 0.0012930199159562866\n",
            "Iteration170, Loss: 0.0012166024389232565\n",
            "Iteration171, Loss: 0.0011447012347829176\n",
            "Iteration172, Loss: 0.0010770493918072417\n",
            "Iteration173, Loss: 0.0010133957727514104\n",
            "Iteration174, Loss: 0.0009535040825818078\n",
            "Iteration175, Loss: 0.0008971519913012032\n",
            "Iteration176, Loss: 0.0008441303086153036\n",
            "Iteration177, Loss: 0.0007942422073761319\n",
            "Iteration178, Loss: 0.0007473024929201971\n",
            "Iteration179, Loss: 0.0007031369155886336\n",
            "Iteration180, Loss: 0.0006615815238773228\n",
            "Iteration181, Loss: 0.0006224820558161947\n",
            "Iteration182, Loss: 0.0005856933663174669\n",
            "Iteration183, Loss: 0.0005510788883681015\n",
            "Iteration184, Loss: 0.0005185101260655451\n",
            "Iteration185, Loss: 0.00048786617761505856\n",
            "Iteration186, Loss: 0.00045903328651801555\n",
            "Iteration187, Loss: 0.0004319044192847635\n",
            "Iteration188, Loss: 0.0004063788681050637\n",
            "Iteration189, Loss: 0.0003823618770000461\n",
            "Iteration190, Loss: 0.0003597642900693548\n",
            "Iteration191, Loss: 0.0003385022205262612\n",
            "Iteration192, Loss: 0.00031849673929316324\n",
            "Iteration193, Loss: 0.0002996735820009465\n",
            "Iteration194, Loss: 0.0002819628733046985\n",
            "Iteration195, Loss: 0.0002652988674923804\n",
            "Iteration196, Loss: 0.0002496197044235683\n",
            "Iteration197, Loss: 0.00023486717989212869\n",
            "Iteration198, Loss: 0.00022098652956051694\n",
            "Iteration199, Loss: 0.0002079262256634926\n",
            "Iteration200, Loss: 0.00019563778572677352\n",
            "Final weights:  [-3.3990955  -0.20180899  0.80271349]\n",
            "Final bias:  0.6009044964039992\n"
          ]
        }
      ]
    }
  ]
}