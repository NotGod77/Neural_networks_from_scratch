{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zi08eCveuAmn"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = np.array([1, 2, 3, 4])\n",
        "weights = np.array([\n",
        "    [0.1, 0.2, 0.3, 0.4],\n",
        "    [0.5, 0.6, 0.7, 0.8],\n",
        "    [0.9, 1.0, 1.1, 1.2]\n",
        "])\n",
        "biases = np.array([0.1, 0.2, 0.3])\n",
        "learning_rate = 0.001\n",
        "\n",
        "def Relu(x):\n",
        "  return np.maximum(0,x)\n",
        "def Relu_derivative(x):\n",
        "  return np.where(x>0,1.0,0.0)\n",
        "\n",
        "for iterations in range(200):\n",
        "  #forward pass\n",
        "  z=np.dot(weights,inputs)+biases #(3,)\n",
        "  a=Relu(z) #(3,)\n",
        "  y=np.sum(a) #scalar\n",
        "  loss = y**2 #scalar\n",
        "\n",
        "  dL_dy = 2*y #scalar\n",
        "  dy_da = np.ones_like(a) #(3,)\n",
        "  da_dz = Relu_derivative(z) #(3,)\n",
        "\n",
        "  #till now, multiplication of these three terms has shape (3,) -> dL_dz\n",
        "  dL_dz = dL_dy*dy_da*da_dz\n",
        "\n",
        "  #now, an np.outer operation multiplies each dl_dz member with each of inputs array, to get a (3,4)array\n",
        "  dL_dw = np.outer(dL_dz,inputs)\n",
        "  '''\n",
        "\n",
        "  dL_dz* inputs = outer product [2y*1(z1>0),2y*1(z2>0),2y*1(z3>0)]*[x1,x2,x3,x4]\n",
        "\n",
        "dL_dW = [\n",
        "  [2*y*𝟙(z1>0)*x1, 2*y*𝟙(z1>0)*x2, 2*y*𝟙(z1>0)*x3, 2*y*𝟙(z1>0)*x4],\n",
        "  [2*y*𝟙(z2>0)*x1, 2*y*𝟙(z2>0)*x2, 2*y*𝟙(z2>0)*x3, 2*y*𝟙(z2>0)*x4],\n",
        "  [2*y*𝟙(z3>0)*x1, 2*y*𝟙(z3>0)*x2, 2*y*𝟙(z3>0)*x3, 2*y*𝟙(z3>0)*x4]\n",
        "]\n",
        "or\n",
        "dL_dW = [\n",
        "    [dL_dw11, dL_dw12, dL_dw13, dL_dw14],\n",
        "    [dL_dw21, dL_dw22, dL_dw23, dL_dw24],\n",
        "    [dL_dw31, dL_dw32, dL_dw33, dL_dw34]\n",
        "]\n",
        "\n",
        "   '''\n",
        "  #our final gradient of loss wrt weights is a matrix of (3,4) -> 2y*xi\n",
        "  dL_db = dL_dz #essentially always 2y, all other terms are 1\n",
        "\n",
        "  #update\n",
        "  weights-=learning_rate*dL_dw\n",
        "  biases-=learning_rate*dL_db\n",
        "\n",
        "  print(f\"Iteration: {iterations+1}, Loss: {loss}\")\n",
        "\n",
        "#Final weights and biases\n",
        "print(\"Final weights: \",weights)\n",
        "print(\"Final biases: \",biases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJEucgx8uQDv",
        "outputId": "84dc790e-f470-4c0f-ac66-7716e58ed4cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 1, Loss: 466.56000000000006\n",
            "Iteration: 2, Loss: 309.14078976\n",
            "Iteration: 3, Loss: 204.83545073181696\n",
            "Iteration: 4, Loss: 140.8182193826751\n",
            "Iteration: 5, Loss: 108.06052191699968\n",
            "Iteration: 6, Loss: 82.92305106657953\n",
            "Iteration: 7, Loss: 63.63315923526753\n",
            "Iteration: 8, Loss: 48.83055920132266\n",
            "Iteration: 9, Loss: 37.47139919767418\n",
            "Iteration: 10, Loss: 28.754652430714412\n",
            "Iteration: 11, Loss: 22.06563016367191\n",
            "Iteration: 12, Loss: 16.932635012477895\n",
            "Iteration: 13, Loss: 14.840512693931485\n",
            "Iteration: 14, Loss: 13.057336050679451\n",
            "Iteration: 15, Loss: 11.488418780174012\n",
            "Iteration: 16, Loss: 10.108016333223427\n",
            "Iteration: 17, Loss: 8.89347752268863\n",
            "Iteration: 18, Loss: 7.824872837472455\n",
            "Iteration: 19, Loss: 6.884667416813114\n",
            "Iteration: 20, Loss: 6.057433318678514\n",
            "Iteration: 21, Loss: 5.32959636083938\n",
            "Iteration: 22, Loss: 4.689213380506364\n",
            "Iteration: 23, Loss: 4.1257762575582415\n",
            "Iteration: 24, Loss: 3.630039485555074\n",
            "Iteration: 25, Loss: 3.1938684611287185\n",
            "Iteration: 26, Loss: 2.810106002313336\n",
            "Iteration: 27, Loss: 2.472454905499375\n",
            "Iteration: 28, Loss: 2.1753746138741916\n",
            "Iteration: 29, Loss: 1.9139903017695257\n",
            "Iteration: 30, Loss: 1.6840128830701064\n",
            "Iteration: 31, Loss: 1.481668631091935\n",
            "Iteration: 32, Loss: 1.3036372550544522\n",
            "Iteration: 33, Loss: 1.1469974170361295\n",
            "Iteration: 34, Loss: 1.0091787953947366\n",
            "Iteration: 35, Loss: 0.8879199080552862\n",
            "Iteration: 36, Loss: 0.7812310035829951\n",
            "Iteration: 37, Loss: 0.687361411116477\n",
            "Iteration: 38, Loss: 0.6047708134023655\n",
            "Iteration: 39, Loss: 0.5321039715471908\n",
            "Iteration: 40, Loss: 0.4681684867419666\n",
            "Iteration: 41, Loss: 0.41191523404899866\n",
            "Iteration: 42, Loss: 0.3624211471866072\n",
            "Iteration: 43, Loss: 0.3188740718252532\n",
            "Iteration: 44, Loss: 0.2805594388510181\n",
            "Iteration: 45, Loss: 0.2468485389164351\n",
            "Iteration: 46, Loss: 0.21718820587439186\n",
            "Iteration: 47, Loss: 0.1910917398093484\n",
            "Iteration: 48, Loss: 0.16813092072081634\n",
            "Iteration: 49, Loss: 0.14792898181068598\n",
            "Iteration: 50, Loss: 0.13015442707224115\n",
            "Iteration: 51, Loss: 0.11451559173294901\n",
            "Iteration: 52, Loss: 0.10075585629268477\n",
            "Iteration: 53, Loss: 0.0886494356239809\n",
            "Iteration: 54, Loss: 0.07799767403714583\n",
            "Iteration: 55, Loss: 0.06862578551553851\n",
            "Iteration: 56, Loss: 0.060379985631133434\n",
            "Iteration: 57, Loss: 0.05312496807763904\n",
            "Iteration: 58, Loss: 0.0467416844133022\n",
            "Iteration: 59, Loss: 0.041125390580937476\n",
            "Iteration: 60, Loss: 0.03618392815029437\n",
            "Iteration: 61, Loss: 0.031836212079467595\n",
            "Iteration: 62, Loss: 0.028010900180847065\n",
            "Iteration: 63, Loss: 0.024645222458717243\n",
            "Iteration: 64, Loss: 0.021683951108967602\n",
            "Iteration: 65, Loss: 0.019078494279518517\n",
            "Iteration: 66, Loss: 0.016786098720868715\n",
            "Iteration: 67, Loss: 0.01476914824296401\n",
            "Iteration: 68, Loss: 0.012994546466682442\n",
            "Iteration: 69, Loss: 0.01143317374143173\n",
            "Iteration: 70, Loss: 0.010059409317356252\n",
            "Iteration: 71, Loss: 0.008850710931420008\n",
            "Iteration: 72, Loss: 0.007787244908744308\n",
            "Iteration: 73, Loss: 0.006851560709489224\n",
            "Iteration: 74, Loss: 0.006028304580879819\n",
            "Iteration: 75, Loss: 0.005303967615659622\n",
            "Iteration: 76, Loss: 0.004666664082832435\n",
            "Iteration: 77, Loss: 0.00410593639329562\n",
            "Iteration: 78, Loss: 0.0036125835000228004\n",
            "Iteration: 79, Loss: 0.003178509916994065\n",
            "Iteration: 80, Loss: 0.0027965928794077147\n",
            "Iteration: 81, Loss: 0.002460565465389601\n",
            "Iteration: 82, Loss: 0.0021649137613302567\n",
            "Iteration: 83, Loss: 0.0019047863834238405\n",
            "Iteration: 84, Loss: 0.0016759148707371741\n",
            "Iteration: 85, Loss: 0.001474543643528874\n",
            "Iteration: 86, Loss: 0.0012973683774970255\n",
            "Iteration: 87, Loss: 0.0011414817827304954\n",
            "Iteration: 88, Loss: 0.0010043258976447365\n",
            "Iteration: 89, Loss: 0.0008836501150873324\n",
            "Iteration: 90, Loss: 0.0007774742518588998\n",
            "Iteration: 91, Loss: 0.0006840560556525385\n",
            "Iteration: 92, Loss: 0.0006018626162295437\n",
            "Iteration: 93, Loss: 0.0005295452117138669\n",
            "Iteration: 94, Loss: 0.00046591717725518237\n",
            "Iteration: 95, Loss: 0.0004099344329049093\n",
            "Iteration: 96, Loss: 0.00036067835118478575\n",
            "Iteration: 97, Loss: 0.00031734068321982587\n",
            "Iteration: 98, Loss: 0.0002792102960868692\n",
            "Iteration: 99, Loss: 0.00024566150375025336\n",
            "Iteration: 100, Loss: 0.0002161438001056386\n",
            "Iteration: 101, Loss: 0.000190172825660145\n",
            "Iteration: 102, Loss: 0.00016732241962012735\n",
            "Iteration: 103, Loss: 0.00014721762696825352\n",
            "Iteration: 104, Loss: 0.00012952854578225566\n",
            "Iteration: 105, Loss: 0.00011396491383524246\n",
            "Iteration: 106, Loss: 0.00010027134564845534\n",
            "Iteration: 107, Loss: 8.822314184071818e-05\n",
            "Iteration: 108, Loss: 7.76226020097066e-05\n",
            "Iteration: 109, Loss: 6.82957806426276e-05\n",
            "Iteration: 110, Loss: 6.008963282373209e-05\n",
            "Iteration: 111, Loss: 5.28695029021632e-05\n",
            "Iteration: 112, Loss: 4.6516914911452744e-05\n",
            "Iteration: 113, Loss: 4.0927628483352786e-05\n",
            "Iteration: 114, Loss: 3.6009928355305675e-05\n",
            "Iteration: 115, Loss: 3.1683119403846986e-05\n",
            "Iteration: 116, Loss: 2.787620250875636e-05\n",
            "Iteration: 117, Loss: 2.4526709520114518e-05\n",
            "Iteration: 118, Loss: 2.157967821101559e-05\n",
            "Iteration: 119, Loss: 1.898675039589321e-05\n",
            "Iteration: 120, Loss: 1.6705378415325086e-05\n",
            "Iteration: 121, Loss: 1.4698126966451542e-05\n",
            "Iteration: 122, Loss: 1.293205882267119e-05\n",
            "Iteration: 123, Loss: 1.1378194362774346e-05\n",
            "Iteration: 124, Loss: 1.0011036040919489e-05\n",
            "Iteration: 125, Loss: 8.80814999438679e-06\n",
            "Iteration: 126, Loss: 7.749797923661431e-06\n",
            "Iteration: 127, Loss: 6.818613204344925e-06\n",
            "Iteration: 128, Loss: 5.999315916163863e-06\n",
            "Iteration: 129, Loss: 5.278462112941122e-06\n",
            "Iteration: 130, Loss: 4.644223219297674e-06\n",
            "Iteration: 131, Loss: 4.086191934160694e-06\n",
            "Iteration: 132, Loss: 3.5952114561194955e-06\n",
            "Iteration: 133, Loss: 3.163225228398182e-06\n",
            "Iteration: 134, Loss: 2.7831447378551217e-06\n",
            "Iteration: 135, Loss: 2.4487331987338138e-06\n",
            "Iteration: 136, Loss: 2.1545032125068455e-06\n",
            "Iteration: 137, Loss: 1.895626724504719e-06\n",
            "Iteration: 138, Loss: 1.6678557997948718e-06\n",
            "Iteration: 139, Loss: 1.4674529183147234e-06\n",
            "Iteration: 140, Loss: 1.2911296454616241e-06\n",
            "Iteration: 141, Loss: 1.1359926717815175e-06\n",
            "Iteration: 142, Loss: 9.994963363109655e-07\n",
            "Iteration: 143, Loss: 8.794008545252129e-07\n",
            "Iteration: 144, Loss: 7.737355654488364e-07\n",
            "Iteration: 145, Loss: 6.807665948467607e-07\n",
            "Iteration: 146, Loss: 5.989684038763183e-07\n",
            "Iteration: 147, Loss: 5.269987563400152e-07\n",
            "Iteration: 148, Loss: 4.636766937733644e-07\n",
            "Iteration: 149, Loss: 4.0796315695611795e-07\n",
            "Iteration: 150, Loss: 3.5894393586886055e-07\n",
            "Iteration: 151, Loss: 3.1581466831063177e-07\n",
            "Iteration: 152, Loss: 2.7786764102513465e-07\n",
            "Iteration: 153, Loss: 2.444801767501443e-07\n",
            "Iteration: 154, Loss: 2.1510441663245662e-07\n",
            "Iteration: 155, Loss: 1.892583303474886e-07\n",
            "Iteration: 156, Loss: 1.6651780640630245e-07\n",
            "Iteration: 157, Loss: 1.4650969285977134e-07\n",
            "Iteration: 158, Loss: 1.289056742045843e-07\n",
            "Iteration: 159, Loss: 1.1341688401476149e-07\n",
            "Iteration: 160, Loss: 9.978916489917351e-08\n",
            "Iteration: 161, Loss: 8.779889800154524e-08\n",
            "Iteration: 162, Loss: 7.72493336133102e-08\n",
            "Iteration: 163, Loss: 6.796736268358103e-08\n",
            "Iteration: 164, Loss: 5.980067625299646e-08\n",
            "Iteration: 165, Loss: 5.261526619713125e-08\n",
            "Iteration: 166, Loss: 4.629322627196534e-08\n",
            "Iteration: 167, Loss: 4.073081737599484e-08\n",
            "Iteration: 168, Loss: 3.5836765283401785e-08\n",
            "Iteration: 169, Loss: 3.153076291402601e-08\n",
            "Iteration: 170, Loss: 2.77421525653483e-08\n",
            "Iteration: 171, Loss: 2.4408766481736144e-08\n",
            "Iteration: 172, Loss: 2.14759067363383e-08\n",
            "Iteration: 173, Loss: 1.8895447686540504e-08\n",
            "Iteration: 174, Loss: 1.6625046274292638e-08\n",
            "Iteration: 175, Loss: 1.4627447214131197e-08\n",
            "Iteration: 176, Loss: 1.2869871666684987e-08\n",
            "Iteration: 177, Loss: 1.1323479366681083e-08\n",
            "Iteration: 178, Loss: 9.962895379898372e-09\n",
            "Iteration: 179, Loss: 8.765793722617453e-09\n",
            "Iteration: 180, Loss: 7.712531012094228e-09\n",
            "Iteration: 181, Loss: 6.7858241357822796e-09\n",
            "Iteration: 182, Loss: 5.970466650935363e-09\n",
            "Iteration: 183, Loss: 5.253079260026447e-09\n",
            "Iteration: 184, Loss: 4.621890268470638e-09\n",
            "Iteration: 185, Loss: 4.066542421370032e-09\n",
            "Iteration: 186, Loss: 3.5779229501800417e-09\n",
            "Iteration: 187, Loss: 3.1480140401832544e-09\n",
            "Iteration: 188, Loss: 2.769761265155462e-09\n",
            "Iteration: 189, Loss: 2.4369578305909104e-09\n",
            "Iteration: 190, Loss: 2.1441427254992643e-09\n",
            "Iteration: 191, Loss: 1.8865111121738734e-09\n",
            "Iteration: 192, Loss: 1.6598354829854144e-09\n",
            "Iteration: 193, Loss: 1.4603962906971435e-09\n",
            "Iteration: 194, Loss: 1.284920913992261e-09\n",
            "Iteration: 195, Loss: 1.1305299566400613e-09\n",
            "Iteration: 196, Loss: 9.946899991675513e-10\n",
            "Iteration: 197, Loss: 8.751720276272285e-10\n",
            "Iteration: 198, Loss: 7.700148574790648e-10\n",
            "Iteration: 199, Loss: 6.774929522716155e-10\n",
            "Iteration: 200, Loss: 5.960881091030875e-10\n",
            "Final weights:  [[-0.00698895 -0.01397789 -0.02096684 -0.02795579]\n",
            " [ 0.25975286  0.11950572 -0.02074143 -0.16098857]\n",
            " [ 0.53548461  0.27096922  0.00645383 -0.25806156]]\n",
            "Final biases:  [-0.00698895 -0.04024714 -0.06451539]\n"
          ]
        }
      ]
    }
  ]
}