{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vxzXZKp9jeoz"
      },
      "outputs": [],
      "source": [
        "# Cross entropy loss\n",
        "# -summation(True_label*log(probability))\n",
        "\n",
        "#Advanced indexing in python = matrix[[0,1,2],[0,1,1]]\n",
        "#Selects the 0th index from row 0, 1st index from row 1, 1st index from row 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "WLXSd5FDNeMI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#as the cross entropy loss can only be calculated using the true label, the others don't matter\n",
        "#we need to extract the probabilities of the required classes only"
      ],
      "metadata": {
        "id": "B4kaWVbRNo0s"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###When class targets are provided"
      ],
      "metadata": {
        "id": "lI0i8O_yN4gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Suppose we got these probabilistic values as output from a 3 neuron final layer with 3 batches of data\n",
        "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
        " [0.1, 0.5, 0.4],\n",
        " [0.02, 0.9, 0.08]])\n",
        "class_targets = np.array([0,1,1])\n",
        "#Extracting the required softmax outputs using advanced indexing\n",
        "print(softmax_outputs[[0,1,2],class_targets])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEm3MUO2OEGH",
        "outputId": "ca097e06-6337-4913-c498-f7a4ac481b48"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7 0.5 0.9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(softmax_outputs[range(len(softmax_outputs)),class_targets]) #here this is equal to range(3) = 0,1,2, then it pairs up these with the class targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQmdBKt6Pm5Y",
        "outputId": "b78b284b-9ef1-487d-dacc-d6a362bfef37"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7 0.5 0.9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we can calculate losses easily\n",
        "output_list=softmax_outputs[[0,1,2],class_targets]\n",
        "loss = -np.log(output_list)\n",
        "average_loss = np.mean(loss)\n",
        "print(loss,average_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34GwnvW6PP0u",
        "outputId": "511ba8d0-f45c-489f-ca96-0e099327ddc7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.35667494 0.69314718 0.10536052] 0.38506088005216804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###When class targets are one-hot encoded"
      ],
      "metadata": {
        "id": "AKHzGnP4PBiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Just multiply the class encoded matrix elementwise to the output matrix\n",
        "y_true_check = np.array([\n",
        "    [1, 0, 0],\n",
        "    [0, 1, 0],\n",
        "    [0, 1, 0]\n",
        "])\n",
        "\n",
        "y_pred_clipped_check = np.array([\n",
        "    [0.7, 0.1, 0.2],\n",
        "    [0.1, 0.5, 0.4],\n",
        "    [0.02, 0.9, 0.08]\n",
        "])\n",
        "\n",
        "output_matrix = y_true_check*y_pred_clipped_check\n",
        "prob_matrix= np.sum(output_matrix,axis =1)\n",
        "print(prob_matrix)\n",
        "loss_matrix = -np.log(prob_matrix)\n",
        "print(loss_matrix)\n",
        "print(\"Average loss: \",np.mean(loss_matrix))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JgE4Jz5O_kP",
        "outputId": "6e1e4903-40fc-4f39-b8d8-35a1b2d5b64c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7 0.5 0.9]\n",
            "[0.35667494 0.69314718 0.10536052]\n",
            "Average loss:  0.38506088005216804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Parent loss class, to calculate the loss per batch, and mean.\n",
        "#the forward method will be in the child class, which will calculate the loss per batch\n",
        "class Loss:\n",
        "  def calculate(self,x,y):\n",
        "    sample_losses = self.forward(x,y)\n",
        "    return np.mean(sample_losses)"
      ],
      "metadata": {
        "id": "Q-1WZtfxR4e7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Crossentropyloss(Loss):\n",
        "  def forward(self,y_pred,y_target):\n",
        "    samples = len(y_pred)\n",
        "    y_pred_clipped = np.clip(y_pred,1e-7,1-1e-7)\n",
        "\n",
        "    if len(y_target.shape)==1:\n",
        "      required_confidences = y_pred_clipped[range(samples),y_target]\n",
        "    elif len(y_target.shape)==2:\n",
        "      required_confidences= np.sum(y_pred_clipped*y_target,axis=1)\n",
        "\n",
        "    losses = -np.log(required_confidences)\n",
        "    return losses"
      ],
      "metadata": {
        "id": "g3j75ZqWBSz8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applying these classes to our above softmax outputs and class target labels\n",
        "loss_function = Crossentropyloss()\n",
        "loss = loss_function.calculate(softmax_outputs,class_targets) #gets the final mean loss\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMVLgKDHCRME",
        "outputId": "41d6a08c-3e18-40c4-cb30-d24a68f0fc0a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.38506088005216804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy\n",
        "predictions = np.argmax(softmax_outputs,axis=1)\n",
        "if(class_targets.shape)==2:\n",
        "  class_targets = np.argmax(class_targets,axis=1)\n",
        "accuracy = np.mean(predictions==class_targets)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ2c4GAzJYzT",
        "outputId": "d0254d4f-b520-4eec-d826-d301deab1cd7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hence, our flow becomes\n",
        "# Denselayer1->Relu->Denselayer2->Softmax->lossfunctionevaluation"
      ],
      "metadata": {
        "id": "K8PPOwKfV5kl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}